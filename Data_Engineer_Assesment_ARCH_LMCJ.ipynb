{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Webscrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Scrapping only one page containing 20 books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (i) Identify the website, open and extract the info (soup) within it: \n",
    "URL = \"https://books.toscrape.com/catalogue/page-\"\n",
    "req = requests.get(URL+str(1)+'.html')\n",
    "soup = BeautifulSoup(req.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ii) Extract the needed info nested in the li section with an specific class:\n",
    "info_li = soup.find_all('li', attrs={'class': 'col-xs-6 col-sm-4 col-md-3 col-lg-3'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (iii) Validating the 20 books info was extracted:\n",
    "len(info_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (iv) Extract the section where the website and the titles are:\n",
    "# Option A:\n",
    "titles_ini = []\n",
    "for i in range(0,len(info_li)):\n",
    "    title = soup.find_all('li', class_= 'col-xs-6 col-sm-4 col-md-3 col-lg-3')[i].find(\"article\", class_=\"product_pod\").find(\"h3\").find(\"a\")\n",
    "    titles_ini.append(title)\n",
    "# Option B: \n",
    "#titles_ini = []\n",
    "#for i in range(0,len(info_li)):\n",
    "#    title = soup.find_all('li', class_= 'col-xs-6 col-sm-4 col-md-3 col-lg-3')[i].find(\"article\", class_=\"product_pod\").find(\"h3\").find(\"a\", title=True)\n",
    "#    titles_ini.append(title)  \n",
    "\n",
    "# Validating:\n",
    "# titles_ini\n",
    "# A list with the website and the full name of the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (iv) Extracting the book full name:\n",
    "title_final = []\n",
    "for item in titles_ini:\n",
    "    #print( item['title'])\n",
    "    title_final.append(item['title'])\n",
    "# title_final    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (v) Extracting the book reference (website):\n",
    "title_ref = []\n",
    "for item in titles_ini:\n",
    "    #print( item['href'])\n",
    "    title_ref.append(item['href'])\n",
    "# title_ref[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Poetry'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (vi) Extracting the category:\n",
    "URL2 = \"https://books.toscrape.com/catalogue/\"\n",
    "req2 = requests.get(URL2+title_ref[0])\n",
    "soup2 = BeautifulSoup(req2.text, \"html.parser\")\n",
    "category = soup2.find('ul', class_ = 'breadcrumb')#.findAll('li')#.find(\"a\", href = True)\n",
    "text = list(category.descendants)\n",
    "text[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (vii) Another way to do Name Scraping:\n",
    "Name = soup2.find('div', class_ = 'col-sm-6 product_main')\n",
    "text = list(Name.descendants)\n",
    "Name_final = (text[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a897fe39b1053632 In stock (22 available) 0 Â£51.77\n"
     ]
    }
   ],
   "source": [
    "# (viii) Extracting UPC:\n",
    "UPC_Avai_Review = soup2.find('table', class_ = 'table table-striped')\n",
    "text = list(UPC_Avai_Review.descendants)\n",
    "print(text[6], text[47], text[56], text[22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ix) Extracting the Availability:\n",
    "Avai_num = int(re.findall(r'\\d+', text[47])[0])\n",
    "# Avai_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "# (x) text line finder: \n",
    "count = 0\n",
    "for i in range(0,len(text)):\n",
    "    count = 1 + count\n",
    "    if text[i] == 'Â£51.77':\n",
    "       print(count-1)\n",
    "       break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Three'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (xi) Rating Scraping:\n",
    "rating = soup2.find('div', class_= 'col-sm-6 product_main').find(\"p\").find_next(\"p\").find_next(\"p\", class_= True)\n",
    "# text = list(rating.descendants)\n",
    "temp = rating['class']\n",
    "rating_final = temp[1]\n",
    "rating_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Scraping all 50 webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://books.toscrape.com/catalogue/page-\"\n",
    "URL2 = \"https://books.toscrape.com/catalogue/\"\n",
    "\n",
    "# (i) main variables\n",
    "title_final = []\n",
    "title_ref = []\n",
    "rating_final = []\n",
    "#instock_final = []\n",
    "price_final = []\n",
    "price_currency_final = []\n",
    "\n",
    "# (ii) temporal variables\n",
    "temp = []\n",
    "\n",
    "# (iii) Webscraping algorithm:\n",
    "a = 51\n",
    "for page in range (1,a):\n",
    "    req = requests.get(URL+str(page)+'.html')\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "    info_li = soup.find_all('li', attrs={'class': 'col-xs-6 col-sm-4 col-md-3 col-lg-3'})\n",
    "    len(info_li)\n",
    "    if page < a:\n",
    "       titles_ini = []\n",
    "       rating_ini = []\n",
    "              \n",
    "       for i in range(0,len(info_li)):\n",
    "           title = soup.find_all('li', class_= 'col-xs-6 col-sm-4 col-md-3 col-lg-3')[i].find(\"article\", class_=\"product_pod\").find(\"h3\").find(\"a\")\n",
    "           titles_ini.append(title)\n",
    "           \n",
    "           rating = soup.find_all('li', class_= 'col-xs-6 col-sm-4 col-md-3 col-lg-3')[i].find(\"article\", class_=\"product_pod\").find(\"p\")\n",
    "           rating_ini.append(rating)\n",
    "           \n",
    "           #instock = soup.find_all('li', class_= 'col-xs-6 col-sm-4 col-md-3 col-lg-3')[i].find(\"article\", class_=\"product_pod\").find('div', class_=\"product_price\").find(\"p\", class_=\"instock availability\").get_text()\n",
    "           #instock_final.append(re.findall((\"In stock\"), instock)[0])\n",
    "                      \n",
    "           price = soup.find_all('li', class_= 'col-xs-6 col-sm-4 col-md-3 col-lg-3')[i].find(\"article\", class_=\"product_pod\").find('div', class_=\"product_price\").find(\"p\", class_=\"price_color\").get_text()\n",
    "           price_currency = re.findall((\"\\D\"), price)\n",
    "           price_currency_final.append(price_currency[1])\n",
    "           price = re.findall((\"([0-9]+[,.]+[0-9]+)\"), price)\n",
    "           price_final.append(float(price[0]))\n",
    "                  \n",
    "       for item in titles_ini:\n",
    "            title_final.append(item['title']) \n",
    "       for item in titles_ini:\n",
    "            title_ref.append(item['href'])\n",
    "       for item in rating_ini:\n",
    "            temp = item['class']\n",
    "            rating_final.append([temp[1]][0])\n",
    "            #rating_final.append([temp[index] for index in [1]][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_final = []\n",
    "UPC_final = []\n",
    "Avai_final = []\n",
    "Review_final =[]\n",
    "\n",
    "for i in range(0, len(title_ref)):      \n",
    "    req2 = requests.get(URL2+title_ref[i])\n",
    "    soup2 = BeautifulSoup(req2.text, \"html.parser\")\n",
    "    \n",
    "    # Category Scraping:\n",
    "    category = soup2.find('ul', class_ = 'breadcrumb')#.findAll('li')#.find(\"a\", href = True)\n",
    "    text = list(category.descendants)\n",
    "    category_final.append(text[16])\n",
    "    \n",
    "    # UPC, Availability and Review Scraping:\n",
    "    UPC_Avai_Review = soup2.find('table', class_ = 'table table-striped')\n",
    "    text = list(UPC_Avai_Review.descendants)\n",
    "    UPC_final.append(text[6]) \n",
    "    Avai_final.append(int((re.findall(r'\\d+', text[47]))[0]))\n",
    "    Review_final.append(int(re.findall(r'\\d+',text[56])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(rating_final)):\n",
    "    if rating_final[i] == 'One':\n",
    "        rating_final[i] = rating_final[i].replace('One', '1')\n",
    "    elif rating_final[i] == 'Two':\n",
    "        rating_final[i] = rating_final[i].replace('Two', '2')\n",
    "    elif rating_final[i] ==  'Three':\n",
    "        rating_final[i] = rating_final[i].replace('Three', '3')   \n",
    "    elif rating_final[i] ==  'Four':\n",
    "        rating_final[i] = rating_final[i].replace('Four', '4') \n",
    "    else:\n",
    "        rating_final[i] = rating_final[i].replace('Five', '5')  \n",
    "    rating_final[i] = int(rating_final[i])   \n",
    "\n",
    "# Validating:    \n",
    "# print(rating_final)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating:\n",
    "# UPC_final\n",
    "# category_final\n",
    "# Avai_final\n",
    "# Review_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000 1000 1000 1000 1000 1000 1000 1000\n"
     ]
    }
   ],
   "source": [
    "print(len(title_final), len(title_ref),\n",
    "      len(rating_final),\n",
    "      len(price_final),\n",
    "      len(price_currency_final),\n",
    "      len(category_final), len(UPC_final), len(Avai_final), len(Review_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data ={'Name': title_final, 'Rating': rating_final, 'Price Currency': price_currency_final,'Price': price_final,\n",
    "#        'UPC': UPC_final, '# Available': Avai_final, '# of Review': Review_final, 'Category':category_final}\n",
    "data ={'Name': title_final, 'Rating': rating_final, 'Price Currency': price_currency_final,'Price': price_final,\n",
    "       'UPC': UPC_final, 'Available': Avai_final, 'Review': Review_final, 'Category':category_final}\n",
    "DF =  pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Name            1000 non-null   object \n",
      " 1   Rating          1000 non-null   int64  \n",
      " 2   Price Currency  1000 non-null   object \n",
      " 3   Price           1000 non-null   float64\n",
      " 4   UPC             1000 non-null   object \n",
      " 5   Available       1000 non-null   int64  \n",
      " 6   Review          1000 non-null   int64  \n",
      " 7   Category        1000 non-null   object \n",
      "dtypes: float64(1), int64(3), object(4)\n",
      "memory usage: 62.6+ KB\n"
     ]
    }
   ],
   "source": [
    "#DF = DF.style.set_properties(**{'text-align': 'left'})\n",
    "#display(DF)\n",
    "DF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Price</th>\n",
       "      <th>Available</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.923000</td>\n",
       "      <td>35.07035</td>\n",
       "      <td>8.585000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.434967</td>\n",
       "      <td>14.44669</td>\n",
       "      <td>5.654622</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>22.10750</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>35.98000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>47.45750</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>59.99000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Rating       Price    Available  Review\n",
       "count  1000.000000  1000.00000  1000.000000  1000.0\n",
       "mean      2.923000    35.07035     8.585000     0.0\n",
       "std       1.434967    14.44669     5.654622     0.0\n",
       "min       1.000000    10.00000     1.000000     0.0\n",
       "25%       2.000000    22.10750     3.000000     0.0\n",
       "50%       3.000000    35.98000     7.000000     0.0\n",
       "75%       4.000000    47.45750    14.000000     0.0\n",
       "max       5.000000    59.99000    22.000000     0.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF.to_csv(r'C:/Users/luisc/Downloads/2022_ARCH_Partners/DF.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1:\n",
    "# Function parse:\n",
    "# def parse(URL):\n",
    "            \n",
    "#     # Request and parsing the URL:      \n",
    "#     req2 = requests.get(URL)\n",
    "#     soup2 = BeautifulSoup(req2.text, \"html.parser\")\n",
    "    \n",
    "#     # Name Scraping:\n",
    "#     Name = soup2.find('div', class_ = 'col-sm-6 product_main')\n",
    "#     text = list(Name.descendants)\n",
    "#     Name_final = (text[2])[0]\n",
    "    \n",
    "#     # Rating Scraping:\n",
    "#     rating = soup2.find('div', class_= 'col-sm-6 product_main').find(\"p\").find_next(\"p\").find_next(\"p\", class_= True)\n",
    "#     temp = rating['class']\n",
    "#     Rating_final = temp[1]\n",
    "#     if Rating_final == 'One':\n",
    "#         Rating_final = Rating_final.replace('One', '1')\n",
    "#     elif Rating_final == 'Two':\n",
    "#         Rating_final = Rating_final.replace('Two', '2')\n",
    "#     elif Rating_final ==  'Three':\n",
    "#         Rating_final = Rating_final.replace('Three', '3')   \n",
    "#     elif Rating_final ==  'Four':\n",
    "#         Rating_final = Rating_final.replace('Four', '4') \n",
    "#     else:\n",
    "#         Rating_final = Rating_final.replace('Five', '5')  \n",
    "    \n",
    "#     Rating_final = int(Rating_final)  \n",
    "    \n",
    "    \n",
    "#     # UPC, Availability and Review Scraping:\n",
    "#     UPC_Avai_Review_Price = soup2.find('table', class_ = 'table table-striped')\n",
    "#     text = list(UPC_Avai_Review_Price.descendants)\n",
    "#     UPC_final = text[6] \n",
    "#     Avai_final = int((re.findall(r'\\d+', text[47]))[0])\n",
    "#     Review_final = int(re.findall(r'\\d+',text[56])[0])\n",
    "#     Price_final = float(re.findall(r'\\d+',text[22])[0])\n",
    "    \n",
    "#     # Category Scraping:\n",
    "#     category = soup2.find('ul', class_ = 'breadcrumb')#.findAll('li')#.find(\"a\", href = True)\n",
    "#     text = list(category.descendants)\n",
    "#     Category_final= text[16]\n",
    "    \n",
    "#     return Name_final, Rating_final, Price_final, UPC_final, Avai_final, Review_final, Category_final\n",
    "\n",
    "    #      '; # of Review: ', Review_final, '; Category: ', Category_final)\n",
    "    #data ={'Name': Name_final, 'Price': Price_final,\n",
    "    #   'UPC': UPC_final, '# Avilable': Avai_final, '# of Review': Review_final, 'Category':Category_final}\n",
    "    # DF =  pd.DataFrame(data)\n",
    "    #print('Name: ', Name_final, '; Price: ', Price_final, '; UPC: ', UPC_final, '; # Avilable: ', Avai_final,\n",
    "    #      '; # of Review: ', Review_final, '; Category: ', Category_final)\n",
    "    # DF =  pd.DataFrame(data))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2:\n",
    "def parse(URL):\n",
    "                \n",
    "    # Request and parsing the URL:      \n",
    "    req2 = requests.get(URL)\n",
    "    soup2 = BeautifulSoup(req2.text, \"html.parser\")\n",
    "    \n",
    "    # Name Scraping:\n",
    "    Name = soup2.find('div', class_ = 'col-sm-6 product_main')\n",
    "    text = list(Name.descendants)\n",
    "    Name_final = text[2]\n",
    "    \n",
    "    # Rating Scraping:\n",
    "    rating = soup2.find('div', class_= 'col-sm-6 product_main').find(\"p\").find_next(\"p\").find_next(\"p\", class_= True)\n",
    "    temp = rating['class']\n",
    "    Rating_final = temp[1]\n",
    "    if Rating_final == 'One':\n",
    "        Rating_final = Rating_final.replace('One', '1')\n",
    "    elif Rating_final == 'Two':\n",
    "        Rating_final = Rating_final.replace('Two', '2')\n",
    "    elif Rating_final ==  'Three':\n",
    "        Rating_final = Rating_final.replace('Three', '3')   \n",
    "    elif Rating_final ==  'Four':\n",
    "        Rating_final = Rating_final.replace('Four', '4') \n",
    "    else:\n",
    "        Rating_final = Rating_final.replace('Five', '5')  \n",
    "    \n",
    "    Rating_final = int(Rating_final)  \n",
    "    \n",
    "    \n",
    "    # UPC, Availability and Review Scraping:\n",
    "    UPC_Avai_Review_Price = soup2.find('table', class_ = 'table table-striped')\n",
    "    text = list(UPC_Avai_Review_Price.descendants)\n",
    "    UPC_final = text[6] \n",
    "    Avai_final = int((re.findall(r'\\d+', text[47]))[0])\n",
    "    Review_final = int(re.findall(r'\\d+',text[56])[0])\n",
    "    Price_final = float(re.findall(r'\\d+',text[22])[0])\n",
    "    \n",
    "    # Category Scraping:\n",
    "    category = soup2.find('ul', class_ = 'breadcrumb')#.findAll('li')#.find(\"a\", href = True)\n",
    "    text = list(category.descendants)\n",
    "    Category_final= text[16]\n",
    "    \n",
    "    return Name_final, Rating_final, Price_final, UPC_final, Avai_final, Review_final, Category_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# books_links:\n",
    "URL3 = \"https://books.toscrape.com/catalogue/\"\n",
    "\n",
    "API_ = []\n",
    "books_links = []\n",
    "for t in title_ref:\n",
    "    API_ = '{}{}'.format(URL3,t)\n",
    "    books_links.append(API_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(books_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('A Light in the Attic', 3, 51.0, 'a897fe39b1053632', 22, 0, 'Poetry')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validating:\n",
    "print(books_links[0])\n",
    "parse(books_links[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the python file attached with all the multiprocessing code: multiprocessing_pool_books_case.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) SQLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading DF into the data base:\n",
    "db = \"books.sqlite\"\n",
    "conn = sqlite3.connect(db)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luisc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\generic.py:2872: UserWarning: The spaces in these column names will not be changed. In pandas versions < 0.14, spaces were converted to underscores.\n",
      "  sql.to_sql(\n"
     ]
    }
   ],
   "source": [
    "# Saving DF in sqlite database:\n",
    "DF.to_sql(name='DF2', con=conn)\n",
    "# Note: Run it once. \n",
    "# After Loading in the db it does not need to be reloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Novels', 1, 5.0, 9),\n",
       " ('Erotica', 1, 5.0, 15),\n",
       " ('Adult Fiction', 1, 5.0, 3),\n",
       " ('Christian Fiction', 6, 4.166666666666667, 55),\n",
       " ('Health', 4, 3.75, 49),\n",
       " ('Art', 8, 3.625, 72),\n",
       " ('Poetry', 19, 3.526315789473684, 235),\n",
       " ('Humor', 10, 3.4, 84),\n",
       " ('Spirituality', 6, 3.3333333333333335, 69),\n",
       " ('Young Adult', 54, 3.2962962962962963, 464),\n",
       " ('Historical Fiction', 26, 3.230769230769231, 194),\n",
       " ('Fiction', 65, 3.1846153846153844, 588),\n",
       " ('New Adult', 6, 3.1666666666666665, 49),\n",
       " ('Music', 13, 3.1538461538461537, 111),\n",
       " ('Religion', 7, 3.142857142857143, 60),\n",
       " ('Womens Fiction', 17, 3.1176470588235294, 91),\n",
       " ('Fantasy', 48, 3.0833333333333335, 372),\n",
       " ('Suspense', 1, 3.0, 8),\n",
       " ('Sports and Games', 5, 3.0, 10),\n",
       " ('Historical', 2, 3.0, 20),\n",
       " ('Autobiography', 9, 3.0, 57),\n",
       " ('Sequential Art', 75, 2.973333333333333, 686),\n",
       " ('History', 18, 2.9444444444444446, 181),\n",
       " ('Mystery', 32, 2.9375, 290),\n",
       " ('Science', 14, 2.9285714285714284, 105),\n",
       " ('Business', 12, 2.9166666666666665, 133),\n",
       " ('Food and Drink', 30, 2.9, 319),\n",
       " ('Nonfiction', 110, 2.881818181818182, 975),\n",
       " ('Default', 152, 2.835526315789474, 1345),\n",
       " ('Add a comment', 67, 2.7611940298507465, 516),\n",
       " ('Travel', 11, 2.727272727272727, 89),\n",
       " ('Thriller', 11, 2.727272727272727, 119),\n",
       " ('Horror', 17, 2.7058823529411766, 136),\n",
       " ('Christian', 3, 2.6666666666666665, 37),\n",
       " ('Romance', 35, 2.6285714285714286, 269),\n",
       " ('Childrens', 29, 2.6206896551724137, 229),\n",
       " ('Self Help', 5, 2.6, 59),\n",
       " ('Classics', 19, 2.473684210526316, 55),\n",
       " ('Philosophy', 11, 2.3636363636363638, 82),\n",
       " ('Politics', 3, 2.3333333333333335, 45),\n",
       " ('Contemporary', 3, 2.3333333333333335, 33),\n",
       " ('Science Fiction', 16, 2.25, 125),\n",
       " ('Biography', 5, 2.2, 43),\n",
       " ('Parenting', 1, 2.0, 3),\n",
       " ('Academic', 1, 2.0, 5),\n",
       " ('Psychology', 7, 1.7142857142857142, 52),\n",
       " ('Short Stories', 1, 1.0, 8),\n",
       " ('Paranormal', 1, 1.0, 1),\n",
       " ('Cultural', 1, 1.0, 15),\n",
       " ('Crime', 1, 1.0, 15)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute('SELECT category, count(category) as count_books_per_categ, avg(rating) as avg_rating, sum(available) Sum_Avai  From DF2 Group By Category Order by avg_rating DESC' )\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98171255d14943fb312a3ced32bc41f1ba1082196510a3a27b46e64306024b66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
